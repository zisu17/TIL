## 스파크에서 UDF 활용 모듈 실행 시에 module을 찾지 못하는 에러
### Issue
가명처리 모듈을 anonymization 폴더에 넣어두고 main.py에서 모듈을 불러와 테스트하고 있었는데 UDF를 활용한 모듈만 유독 위치를 못 찾는 이슈가 발생

```python
from pyspark.sql import SparkSession
from suppression import general, partial

spark = SparkSession.builder.appName("Practice").getOrCreate()
datas = spark.read.csv("./anonymization/data/privacy_sample.csv", header=True, inferSchema=True)

general(datas, '국적').show()
```

![image](https://user-images.githubusercontent.com/108858121/233878155-ba785749-a4e8-4532-99b3-af220c6a5c4a.png)

이렇게 UDF를 사용하지 않는 general 모듈은 잘 불러와서 국적이라는 컬럼을 삭제하도록 샘플 데이터가 만들어지는 것을 볼 수 있다.


```python
from pyspark.sql import SparkSession
from suppression import general, partial

spark = SparkSession.builder.appName("Practice").getOrCreate()
datas = spark.read.csv("./anonymization/data/privacy_sample.csv", header=True, inferSchema=True)

partial(datas, '국적', [1,2]).show()
```
<br>

![image](https://user-images.githubusercontent.com/108858121/233878490-9fd1e0ac-cc73-474d-8e0a-f844ab8132e2.png)

UDF를 활용해 부분 삭제하는 모듈을 실행하면 ModuleNotFoundError: No module named 'suppression' 에러가 뜬다.
해당 에러는 PySpark의 worker 프로세스가 'suppression' 모듈을 찾는데 실패했기 때문에 나타난다.
suppression 모듈을 각 worker 노드에서 사용할 수 있도록 PySpark에 배포하고 SparkConf 객체를 사용하여 필요한 파일이나 디렉토리를 PySpark에 정확하게 전달할 수 있도록 코드를 수정하였다.

```python
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from suppression import general, partial

conf = (SparkConf()
        .setAppName("Anonymization")
        .set("spark.submit.pyFiles", "anonymization/suppression.py"))
sc = SparkContext(conf=conf)
spark = SparkSession.builder.config(conf=conf).getOrCreate()
datas = spark.read.csv("./anonymization/data/privacy_sample.csv", header=True, inferSchema=True)
partial(datas, '국적', [1,2]).show()
```
![image](https://user-images.githubusercontent.com/108858121/233879110-ad2868da-929d-4944-a443-f9b21d0f0553.png)

원하던대로 국적 컬럼에서 1,2번 인덱스가 부분삭제 되어 나타나는 것을 볼 수 있다. 방금 수정한 옵션을 conf 파일에 적용하려면 spark-defaults.conf 파일을 찾으면 된다.
UDF를 활용하는 모듈들을 모두 spark-defaults.conf에 적용하여 사용하면 SparkSession을 만들 때 굳이 SparkContext를 지정하지 않아도 적용이 된다.

```
spark.master                     
spark.cores.max                  
spark.serializer                 
spark.driver.memory              
spark.executor.memory            
spark.submit.pyFiles   suppression.py,TwoWayEncryption.py
```

맨 마지막에 spark.submit.pyFiles 옵션을 추가하면 해결 완료~
